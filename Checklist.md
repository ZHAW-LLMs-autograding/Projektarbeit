# ASAG-Pipeline To-Do-Liste (Modellweise)

---

## BERT 2.0

- [ ] **TODO 1 â€“ Datensplit & Setup**

  * Datensplit (Train 80 / Val 10 / Test 10) erstellen
  * Seed = 42 setzen
  * Configs & Checkpoint-Ordner anlegen
  * LoRA / PEFT-Pfad vorbereiten

- [ ] **TODO 2 â€“ Training (Fine-Tuning)**

  * Modell trainieren (ASAG2024, 5-Klassen-Labels)
  * Trainingszeit & GPU-Verbrauch loggen
  * Checkpoints speichern (epochweise)
  * Logs sichern (`train.log`, `wandb.csv`)

- [ ] **TODO 3 â€“ Evaluation**

  * Ladezeit â†’ GPU messen
  * Inferencezeit (einzelne Bewertung) messen
  * Bewertungszeit (10 % Testset) messen
  * Vorhersagen speichern (`preds_bert2.jsonl`)

- [ ] **TODO 4 â€“ Bewertungsmetriken**

  * RMSE, wRMSE, MAE berechnen
  * LÃ¤ngenanalyse (50 / 100 / 200 / max Tokens)
  * wRMSE-Verlauf je LÃ¤nge plotten

- [ ] **TODO 5 â€“ Ranking & Buckets**

  * Scores â†’ [0, 1] normalisieren
  * GleichmÃ¤ÃŸige Bucket-Verteilung (1 / n_Buckets)
  * Kendall Ï„ pro Data_Source berechnen
  * Rangliste & Heatmap erstellen

- [ ] **TODO 6 â€“ Dokumentation**

  * Ergebnisse + Plots in LaTeX / Overleaf einfÃ¼gen
  * Kurze Analyse (z. B. Einfluss Referenzantwort, Seed-StabilitÃ¤t)

---

## LLaMA (Zero-Shot Baseline)

- [ ] **TODO 1 â€“ Datensplit & Setup**

  * Prompt-Template definieren (â€Grade Onlyâ€œ, â€Baselineâ€œ, â€Lenientâ€œ)

- [ ] **TODO 2 â€“ Inference**

  * Zero-Shot-Evaluation mit 5 Prompts durchfÃ¼hren
  * Zeiten + Kosten (Tokens) messen
  * Ergebnisse speichern

- [ ] **TODO 3 â€“ Bewertungsmetriken**

  * RMSE / wRMSE berechnen
  * Prompt-Vergleich (Baseline vs Lenient vs Strict)
  * wRMSE vs Cost-Plot erstellen

- [ ] **TODO 4 â€“ Ranking**

  * Scores â†’ [0, 1]
  * Buckets & Kendall Ï„

- [ ] **TODO 5 â€“ Dokumentation**

  * Prompt-Vergleich diskutieren (Zero- vs Few-Shot-Effekte)

---

## LLaMA (Fine-Tuned on ASAG)

- [ ] **TODO 1 â€“ Setup**

  * Finetune-Konfiguration (LoRA / QLoRA) vorbereiten
  * Split 80 / 10 / 10 + Seed 42

- [ ] **TODO 2 â€“ Training**

  * Fine-Tuning auf ASAG2024
  * Trainingszeit + VRAM loggen

- [ ] **TODO 3 â€“ Evaluation**

  * Benchmark (10 % Testset)
  * Inference-, Bewertungszeit, Ladezeit

- [ ] **TODO 4 â€“ Metriken**

  * RMSE / wRMSE / MAE
  * LÃ¤ngenabhÃ¤ngigkeit

- [ ] **TODO 5 â€“ Ranking**

  * Normalisierung + Buckets + Kendall Ï„

- [ ] **TODO 6 â€“ Vergleich**

  * Gegen Zero-Shot-LLaMA und GPT-4o Baseline

---

## SentenceTransformer

- [ ] **TODO 1 â€“ Setup**

  * Modell: `all-MiniLM-L6-v2` oder `multi-e5`

- [ ] **TODO 2 â€“ Inference**

  * Cosine Similarity zwischen Student & Reference
  * Ergebnisse speichern

- [ ] **TODO 3 â€“ Metriken**

  * RMSE / wRMSE / MAE
  * LÃ¤ngen- vs. Score-Korrelation

- [ ] **TODO 4 â€“ Ranking**

  * Normalisierung + Buckets + Kendall Ï„

- [ ] **TODO 5 â€“ Dokumentation**

  * Vergleich zu BERT 2.0 und LLMs darstellen

---

## Vektormodell (Semantic Baseline)

- [ ] **TODO 1 â€“ Setup**

  * Embedding-Modell (`nomic-embed-text-v1`)

- [ ] **TODO 2 â€“ Inference**

  * Cosine Similarity (y = sim( ref, stud ))
  * Ergebnisse speichern

- [ ] **TODO 3 â€“ Metriken**

  * RMSE / wRMSE / MAE

- [ ] **TODO 4 â€“ Ranking**

  * Buckets + Kendall Ï„

- [ ] **TODO 5 â€“ Dokumentation**

  * Baseline â†’ Vergleich mit anderen Modellen

---

## ğŸ§¾ Abschluss

- [ ] Alle Modelle in einer Vergleichstabelle zusammenfÃ¼hren
- [ ] Zeit, Leistung, wRMSE, VRAM visualisieren
- [ ] Finales Ranking (Performance vs Effizienz)
- [ ] Diskussion + Fazit â†’ Overleaf Kapitel â€œResultate & Diskussionâ€

---
